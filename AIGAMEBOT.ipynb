{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3m6Qk5NRvziD"
   },
   "source": [
    "# Deep Q-Network implementation with transfer learning\n",
    "\n",
    "This notebook implements a DQN - an approximate q-learning algorithm with experience replay and target networks. Trains the algorithm on openAI's gym, to breakout Atari game, and monitors on the screen.we have also used transfer learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "seT-YHg5vziF",
    "outputId": "c7ebc93f-1279-4fc9-b04c-82a0635daee6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DISPLAY=:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'bash' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8C5cmoBmvziM"
   },
   "outputs": [],
   "source": [
    "#Make necessary imports\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline\n",
    "def make_env():\n",
    "    env = gym.make(\"BreakoutDeterministic-v4\")\n",
    "    \n",
    "    return env\n",
    "\n",
    "#Instatntiate gym Atari-Breakout environment\n",
    "env = make_env()\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RAfMgPNBvziQ"
   },
   "source": [
    "### Processing game image \n",
    "\n",
    "Raw atari images are large, 210x160x3 by default. However, we don't need that level of detail in order to learn them.\n",
    "\n",
    "We can thus save a lot of time by preprocessing game image, including\n",
    "* Resizing to a smaller shape, 64 x 64\n",
    "* Converting to grayscale\n",
    "* Cropping irrelevant image parts (top & bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BzuRB5OEvziV"
   },
   "outputs": [],
   "source": [
    "from gym.core import ObservationWrapper\n",
    "from gym.spaces import Box\n",
    "\n",
    "import cv2\n",
    "\n",
    "class PreprocessAtari(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"A gym wrapper that crops, scales image into the desired shapes and optionally grayscales it.\"\"\"\n",
    "        ObservationWrapper.__init__(self,env)\n",
    "        \n",
    "        self.img_size = (84, 84)\n",
    "        self.observation_space = Box(0.0, 1.0, (self.img_size[0], self.img_size[1], 1))\n",
    "\n",
    "    def observation(self, img):\n",
    "        \"\"\"what happens to each observation\"\"\"\n",
    "        # crop image (top and bottom, top from 34, bottom remove last 16)\n",
    "        img = img[34:-16, :, :]\n",
    "        \n",
    "        # resize image\n",
    "        img = cv2.resize(img, self.img_size)\n",
    "        \n",
    "        img = img.mean(-1,keepdims=True)\n",
    "        \n",
    "        img = img.astype('float32') / 255.\n",
    "              \n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nsb99gIIvzid"
   },
   "source": [
    "### Frame buffer\n",
    "\n",
    "Our agent can only process one observation at a time, so we gotta make sure it contains enough information to find optimal actions. For instance, agent has to react to moving objects so he must be able to measure object's velocity.\n",
    "\n",
    "To do so, we introduce a buffer that stores 4 last images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_IA-czvUwbOn"
   },
   "outputs": [],
   "source": [
    "from gym.spaces.box import Box\n",
    "from gym.core import Wrapper\n",
    "class FrameBuffer(Wrapper):\n",
    "    def __init__(self, env, n_frames=4, dim_order='tensorflow'):\n",
    "        \"\"\"A gym wrapper that reshapes, crops and scales image into the desired shapes\"\"\"\n",
    "        super(FrameBuffer, self).__init__(env)\n",
    "        self.dim_order = dim_order\n",
    "        if dim_order == 'tensorflow':\n",
    "            height, width, n_channels = env.observation_space.shape\n",
    "            \"\"\"Multiply channels dimension by number of frames\"\"\"\n",
    "            obs_shape = [height, width, n_channels * n_frames] \n",
    "        else:\n",
    "            raise ValueError('dim_order should be \"tensorflow\" or \"pytorch\", got {}'.format(dim_order))\n",
    "        self.observation_space = Box(0.0, 1.0, obs_shape)\n",
    "        self.framebuffer = np.zeros(obs_shape, 'float32')\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"resets breakout, returns initial frames\"\"\"\n",
    "        self.framebuffer = np.zeros_like(self.framebuffer)\n",
    "        self.update_buffer(self.env.reset())\n",
    "        return self.framebuffer\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"plays breakout for 1 step, returns frame buffer\"\"\"\n",
    "        new_img, reward, done, info = self.env.step(action)\n",
    "        self.update_buffer(new_img)\n",
    "        return self.framebuffer, reward, done, info\n",
    "    \n",
    "    def update_buffer(self, img):\n",
    "        if self.dim_order == 'tensorflow':\n",
    "            offset = self.env.observation_space.shape[-1]\n",
    "            axis = -1\n",
    "            cropped_framebuffer = self.framebuffer[:,:,:-offset]\n",
    "        self.framebuffer = np.concatenate([img, cropped_framebuffer], axis = axis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVuXynjSvzie",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"BreakoutDeterministic-v4\")\n",
    "    env = PreprocessAtari(env)\n",
    "    env = FrameBuffer(env, n_frames=4, dim_order='tensorflow')\n",
    "    return env\n",
    "\n",
    "#Instatntiate gym Atari-Breakout environment\n",
    "env = make_env()\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "colab_type": "code",
    "id": "MgKEPcj2vzii",
    "outputId": "00693f58-6937-43e7-8957-ef2333276d22"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# review Atari image, and actual observation of the Agent after processing\n",
    "for _ in range(50):\n",
    "    obs, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "\n",
    "plt.title(\"Game image\")\n",
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "plt.show()\n",
    "plt.title(\"Agent observation (4 frames left to right)\")\n",
    "plt.imshow(obs.transpose([0,2,1]).reshape([state_dim[0],-1]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nu8jQfGsvzim"
   },
   "source": [
    "### Building a network\n",
    "\n",
    "We now need to build a neural network that can map images to state q-values. This network will be called on every agent's step so it better not be resnet-152 unless we have an array of GPUs. Instead, we can use strided convolutions with a small number of features to save time and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "colab_type": "code",
    "id": "5sPSFBCyvzio",
    "outputId": "d74efe20-f9e0-4705-b18a-945c14ead4ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Dense, Flatten, InputLayer\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fbNLkHuDvziq"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, Dense, Flatten\n",
    "class DQNAgent:\n",
    "    def __init__(self, name, state_shape, n_actions, epsilon=0, reuse=False):\n",
    "        \"\"\"A simple DQN agent\"\"\"\n",
    "        with tf.variable_scope(name, reuse=reuse):\n",
    "            \n",
    "            self.network = keras.models.Sequential()\n",
    "    \n",
    "            # Keras ignores the first dimension in the input_shape, which is the batch size. \n",
    "            # So just use state_shape for the input shape\n",
    "            self.network.add(Conv2D(32, (8, 8), strides=4, activation='relu',use_bias=False, input_shape=state_shape,kernel_initializer=tf.variance_scaling_initializer(scale=2)))\n",
    "            self.network.add(Conv2D(64, (4, 4), strides=2, activation='relu',use_bias=False,kernel_initializer=tf.variance_scaling_initializer(scale=2)))\n",
    "            self.network.add(Conv2D(64, (3, 3), strides=1, activation='relu',use_bias=False,kernel_initializer=tf.variance_scaling_initializer(scale=2)))\n",
    "            self.network.add(Conv2D(1024, (7, 7), strides=1, activation='relu',use_bias=False,kernel_initializer=tf.variance_scaling_initializer(scale=2)))\n",
    "            self.network.add(Flatten())\n",
    "            self.network.add(Dense(n_actions, activation='linear',kernel_initializer=tf.variance_scaling_initializer(scale=2)))\n",
    "            \n",
    "            # prepare a graph for agent step\n",
    "            self.state_t = tf.placeholder('float32', [None,] + list(state_shape))\n",
    "            self.qvalues_t = self.get_symbolic_qvalues(self.state_t)\n",
    "            \n",
    "        self.weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def get_symbolic_qvalues(self, state_t):\n",
    "        \"\"\"takes agent's observation, returns qvalues. Both are tf Tensors\"\"\"\n",
    "        qvalues = self.network(state_t)\n",
    "        \n",
    "        \n",
    "        assert tf.is_numeric_tensor(qvalues) and qvalues.shape.ndims == 2, \\\n",
    "            \"please return 2d tf tensor of qvalues [you got %s]\" % repr(qvalues)\n",
    "        assert int(qvalues.shape[1]) == n_actions\n",
    "        \n",
    "        return qvalues\n",
    "    \n",
    "    def get_qvalues(self, state_t):\n",
    "        \"\"\"Same as symbolic step except it operates on numpy arrays\"\"\"\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run(self.qvalues_t, {self.state_t: state_t})\n",
    "    \n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "        should_explore = np.random.choice([0, 1], batch_size, p = [1-epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "aKZQAx98vzit",
    "outputId": "2b0769d3-bcf1-44d7-bdce-b9dfa413f711"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0514 11:15:00.740797  7748 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0514 11:15:00.744786  7748 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(\"dqn_agent\", state_dim, n_actions, epsilon=0.5)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b0oplbmws0aQ"
   },
   "outputs": [],
   "source": [
    "#Evaluate agents performance, in a number of games\n",
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
    "    rewards = []\n",
    "    s = env.reset()\n",
    "    for _ in range(n_games):\n",
    "        \n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            env.render()\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
    "            s, r, done, _ = env.step(action)\n",
    "         \n",
    "            reward += r\n",
    "            if done:\n",
    "                s = env.reset()\n",
    "                env.close()\n",
    "                break\n",
    "              \n",
    "        \n",
    "        \n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CcEMOVr0vzi6"
   },
   "source": [
    "### Experience replay\n",
    "#### The interface is fairly simple:\n",
    "* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
    "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n",
    "* `len(exp_replay)` - returns number of elements stored in replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VdfJ8uo_w0T0"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
    "        for i in idxes:\n",
    "            data = self._storage[i]\n",
    "            obs_t, action, reward, obs_tp1, done = data\n",
    "            obses_t.append(np.array(obs_t, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
    "            dones.append(done)\n",
    "        return np.array(obses_t), np.array(actions), np.array(rewards), np.array(obses_tp1), np.array(dones)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n",
    "        return self._encode_sample(idxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "efn0J9MXvzi9"
   },
   "outputs": [],
   "source": [
    "def play_and_record(agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Play the game for exactly n steps, record every (s,a,r,s', done) to replay buffer. \n",
    "    Whenever game ends, add record with done=True and reset the game.\n",
    "    :returns: return sum of rewards over time\n",
    "    \n",
    "    Note: please do not env.reset() unless env is done.\n",
    "    It is guaranteed that env has done=False when passed to this function.\n",
    "    \"\"\"\n",
    "    # State at the beginning of rollout\n",
    "    s = env.framebuffer\n",
    "    \n",
    "    # Play the game for n_steps as per instructions above\n",
    "    reward = 0.0\n",
    "    for t in range(n_steps):\n",
    "        #env.render()\n",
    "        # get agent to pick action given state s\n",
    "        qvalues = agent.get_qvalues([s])\n",
    "        action = agent.sample_actions(qvalues)[0]\n",
    "        next_s, r, done, _ = env.step(action)\n",
    "        \n",
    "        # add to replay buffer\n",
    "        exp_replay.add(s, action, r, next_s, done)\n",
    "        reward += r\n",
    "        if done:\n",
    "            s = env.reset()\n",
    "        else:\n",
    "            s = next_s\n",
    "    return reward\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C28S0910vzjI"
   },
   "source": [
    "### Target networks\n",
    "\n",
    "We also employ the so called \"target network\" - a copy of neural network weights to be used for reference Q-values:\n",
    "\n",
    "The network itself is an exact copy of agent network, but it's parameters are not trained. Instead, they are moved here from agent's actual network every so often.\n",
    "\n",
    "$$ Q_{reference}(s,a) = r + \\gamma \\cdot \\max _{a'} Q_{target}(s',a') $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kt3BJYRbvzjM"
   },
   "outputs": [],
   "source": [
    "target_network = DQNAgent(\"target_network\", state_dim, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zb9lUnZ0tAes"
   },
   "outputs": [],
   "source": [
    "def load_weigths_into_target_network(agent, target_network):\n",
    "    \"\"\" assign target_network.weights variables to their respective agent.weights values. \"\"\"\n",
    "    assigns = []\n",
    "    for w_agent, w_target in zip(agent.weights, target_network.weights):\n",
    "        assigns.append(tf.assign(w_target, w_agent, validate_shape=True))\n",
    "    tf.get_default_session().run(assigns)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X0IQNb3pvzjb"
   },
   "source": [
    "### Learning with... Q-learning\n",
    "Here we write a function similar to `agent.update` from tabular q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-AbHB-60vzjc"
   },
   "outputs": [],
   "source": [
    "# Create placeholders that will be fed with exp_replay.sample(batch_size)\n",
    "obs_ph = tf.placeholder(tf.float32, shape=(None,) + state_dim)\n",
    "actions_ph = tf.placeholder(tf.int32, shape=[None])\n",
    "rewards_ph = tf.placeholder(tf.float32, shape=[None])\n",
    "next_obs_ph = tf.placeholder(tf.float32, shape=(None,) + state_dim)\n",
    "is_done_ph = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "is_not_done = 1 - is_done_ph\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a-Ss1UJtvzjf"
   },
   "source": [
    "Take q-values for actions agent just took"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1qFprBwvzjg"
   },
   "outputs": [],
   "source": [
    "current_qvalues = agent.get_symbolic_qvalues(obs_ph)\n",
    "current_action_qvalues = tf.reduce_sum(tf.one_hot(actions_ph, n_actions) * current_qvalues, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Op08x3u0vzji"
   },
   "source": [
    "Compute Q-learning TD error:\n",
    "\n",
    "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
    "\n",
    "With Q-reference defined as\n",
    "\n",
    "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
    "\n",
    "Where\n",
    "* $Q_{target}(s',a')$ denotes q-value of next state and next action predicted by __target_network__\n",
    "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
    "* $\\gamma$ is a discount factor defined two cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "HiY2T2mYvzjj",
    "outputId": "d117b2ec-23ce-4b01-b428-531546dedf5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0514 11:15:01.880751  7748 deprecation.py:323] From c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2638\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2639\u001b[1;33m         \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2640\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Operation 'huber_loss/Minimum' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[1;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m       \u001b[0mxla_compile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_XlaCompile\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m       xla_separate_compiled_gradients = op.get_attr(\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2642\u001b[0m       \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2643\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2644\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Operation 'huber_loss/Minimum' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-5b1cef7aa944>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mtrain_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtd_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[0;32m    401\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m         grad_loss=grad_loss)\n\u001b[0m\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[1;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[0mgate_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgate_gradients\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGATE_OP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m         colocate_gradients_with_ops=colocate_gradients_with_ops)\n\u001b[0m\u001b[0;32m    513\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgate_gradients\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGATE_GRAPH\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m       \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[1;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[0mys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_ys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0mgate_gradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maggregation_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         unconnected_gradients)\n\u001b[0m\u001b[0;32m    159\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[1;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[0;32m    729\u001b[0m                 \u001b[1;31m# functions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[1;32m--> 731\u001b[1;33m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[0;32m    732\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m                 \u001b[1;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[1;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[0;32m    401\u001b[0m       \u001b[0mxla_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_XlaScope\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Exit early\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    729\u001b[0m                 \u001b[1;31m# functions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[1;32m--> 731\u001b[1;33m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[0;32m    732\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m                 \u001b[1;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_MinimumGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1269\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_MinimumGrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1270\u001b[0m   \u001b[1;34m\"\"\"Returns grad*(x < y, x >= y) with type of grad.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1271\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_MaximumMinimumGrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mless_equal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_MaximumMinimumGrad\u001b[1;34m(op, grad, selector_op)\u001b[0m\n\u001b[0;32m   1239\u001b[0m   \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1240\u001b[0m   \u001b[0mgdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1241\u001b[1;33m   \u001b[0msx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1242\u001b[0m   \u001b[0msy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m   \u001b[0mgradshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[1;34m(input, name, out_type)\u001b[0m\n\u001b[0;32m    328\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m   \"\"\"\n\u001b[1;32m--> 330\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mshape_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mshape_internal\u001b[1;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moptimize\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[1;34m(input, out_type, name)\u001b[0m\n\u001b[0;32m  10227\u001b[0m   \u001b[0mout_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"out_type\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10228\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m> 10229\u001b[1;33m         \"Shape\", input=input, out_type=out_type, name=name)\n\u001b[0m\u001b[0;32m  10230\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10231\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[0;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 788\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    789\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 instructions)\n\u001b[1;32m--> 507\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3614\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3615\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3616\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3617\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3618\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   2025\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   2026\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 2027\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   2028\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2029\u001b[0m     \u001b[1;31m# Initialize self._outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1839\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1840\u001b[0m   op_desc = c_api.TF_NewOperation(graph._c_graph, compat.as_str(node_def.op),\n\u001b[1;32m-> 1841\u001b[1;33m                                   compat.as_str(node_def.name))\n\u001b[0m\u001b[0;32m   1842\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m     \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_SetDevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# compute q-values for NEXT states with target network\n",
    "next_qvalues_target =  target_network.get_symbolic_qvalues(next_obs_ph)\n",
    "\n",
    "# compute state values by taking max over next_qvalues_target for all actions\n",
    "next_state_values_target = tf.reduce_max(next_qvalues_target, axis=-1)\n",
    "\n",
    "# compute Q_reference(s,a) as per formula above.\n",
    "reference_qvalues = rewards_ph + gamma*next_state_values_target*is_not_done\n",
    "\n",
    "# Define loss function for sgd.\n",
    "# td_loss = (current_action_qvalues - reference_qvalues) ** 2\n",
    "td_loss = tf.reduce_mean(tf.losses.huber_loss(labels=reference_qvalues, predictions=current_action_qvalues))\n",
    "\n",
    "optimizer=tf.train.AdamOptimizer(1e-5)\n",
    "train_step = optimizer.minimize(td_loss, var_list=agent.weights)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3xPdpbz3vzjs"
   },
   "source": [
    "### Main loop\n",
    "\n",
    "It's time to put everything together and see if it learns anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zIXESnjDY6W8"
   },
   "outputs": [],
   "source": [
    "#Uncomment to load stored weights of trained agent.\n",
    "agent.network.load_weights('finalweight.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SwvsqiQavzjw"
   },
   "outputs": [],
   "source": [
    "#Create the buffer and fill it.\n",
    "exp_replay = ReplayBuffer(70000)\n",
    "play_and_record(agent, env, exp_replay, n_steps=10000)\n",
    "\n",
    "# take a sample batch of observations from the buffer\n",
    "def sample_batch(exp_replay, batch_size):\n",
    "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(batch_size)\n",
    "    return {\n",
    "        obs_ph:obs_batch, actions_ph:act_batch, rewards_ph:reward_batch, \n",
    "        next_obs_ph:next_obs_batch, is_done_ph:is_done_batch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OGQPR9Dvvzjs"
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "moving_average = lambda x, span, **kw: DataFrame({'x':np.asarray(x)}).x.ewm(span=span, **kw).mean().values\n",
    "%matplotlib inline\n",
    "\n",
    "mean_rw_history = []\n",
    "td_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "Wee4pdpKvzjz",
    "outputId": "d96e727b-9400-493c-d97f-382eb2abf21f"
   },
   "outputs": [],
   "source": [
    "# ##Train the agent, configure the starting epsilon to one to encourage exploration\n",
    "agent.epsilon=1\n",
    "for i in trange(100000):\n",
    "    \n",
    "    \n",
    "    # play\n",
    "    play_and_record(agent, env, exp_replay, 10)\n",
    "    \n",
    "    # train the network\n",
    "    _, loss_t = sess.run([train_step, td_loss], sample_batch(exp_replay, batch_size=64))\n",
    "    td_loss_history.append(loss_t)\n",
    "    \n",
    "    # adjust agent parameters\n",
    "    if i % 500 == 0:\n",
    "        load_weigths_into_target_network(agent, target_network)\n",
    "        # reduce epsilon in every iteration until it reaches 1%\n",
    "        agent.epsilon = max(agent.epsilon * 0.999, 0.01)\n",
    "\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "      #uncomment to store agent's weights every some iterations\n",
    "        agent.network.save_weights('dqn_model_atari_weights.h5')\n",
    "        mean_rw_history.append(evaluate(make_env(), agent, n_games=3))\n",
    "        \n",
    "    if i % 500 == 0:\n",
    "        # plot mean reward per game and TD loss history\n",
    "        clear_output(True)\n",
    "        print(\"buffer size = %i, epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n",
    "        \n",
    "        plt.subplot(1,2,1)\n",
    "        plt.title(\"mean reward per game\")\n",
    "        plt.plot(mean_rw_history)\n",
    "        plt.grid()\n",
    "\n",
    "        assert not np.isnan(loss_t)\n",
    "        plt.figure(figsize=[12, 4])\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.title(\"TD loss history (moving average)\")\n",
    "        plt.plot(moving_average(np.array(td_loss_history), span=100, min_periods=100))\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q-oXgDyyvzj6"
   },
   "source": [
    "__ How to interpret plots: __\n",
    "\n",
    "\n",
    "This aint no supervised learning so don't expect anything to improve monotonously. \n",
    "* __ TD loss __ is the MSE between agent's current Q-values and target Q-values. It may slowly increase or decrease, it's ok. The \"not ok\" behavior includes going NaN or stayng at exactly zero before agent has perfect performance.\n",
    "* __ mean reward__ is the expected sum of r(s,a) agent gets over the full game session. It will oscillate, but on average it should get higher over time (after a few thousand iterations...). \n",
    " * In basic q-learning implementation it takes 5-10k steps to \"warm up\" agent before it starts to get better.\n",
    "* __ buffer size__ - this one is simple. It should go up and cap at max size.\n",
    "* __ epsilon__ - agent's willingness to explore. If you see that agent's already at 0.01 epsilon before it's average reward is above 0 - __ it means you need to increase epsilon__. Set it back to some 0.2 - 0.5 and decrease the pace at which it goes down.\n",
    "* Also please ignore first 100-200 steps of each plot - they're just oscillations because of the way moving average works.\n",
    "\n",
    "\n",
    "__Training will take time.__ A lot of it actually. An optimistic estimate is to say it's gonna start winning (average reward > 10) after 10k steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AjeGe0fovzj8"
   },
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oYcTnLI_vzj8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Don't forget to reset epsilon back to previous value if you want to go on training\n",
    "agent.epsilon=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kguY2XL9vzj-",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e3ed8f3a5cce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# #configure directory to store videos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#env_monitor = gym.wrappers.Monitor(make_env(),directory=\"videos\",force=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'finalweight.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"BreakoutDeterministic-v4\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#pong-v0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "# #record session on a video\n",
    "import gym.wrappers\n",
    "# #configure directory to store videos\n",
    "#env_monitor = gym.wrappers.Monitor(make_env(),directory=\"videos\",force=True)\n",
    "agent.network.load_weights('finalweight.h5')\n",
    "env = gym.make(\"BreakoutDeterministic-v4\")\n",
    "#pong-v0\n",
    "#Space-invaders\n",
    "\n",
    "obs=env.reset()\n",
    "\n",
    "env = PreprocessAtari(env)\n",
    "env = FrameBuffer(env, n_frames=4, dim_order='tensorflow')\n",
    "sessions = [evaluate(env, agent, n_games=3) for _ in range(9)]\n",
    "env.reset()\n",
    "\n",
    "\n",
    "\n",
    "# sessions = [evaluate(env, agent, n_games=1) for _ in range(1)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "colab_type": "code",
    "id": "Ze8NUT_LuPQs",
    "outputId": "c33c9c20-c2a0-4ab5-e678-1a768f727176"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "dqn_atari_breakout.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
